{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5edcab8-995c-4a30-8a30-6df504b13e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from typing import Dict, Iterator, Any\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "# register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "tqdm.pandas()\n",
    "from enum import StrEnum\n",
    "import math\n",
    "#from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ea3a22d-e0d8-450d-9768-84d4257c2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrigDataSchema(StrEnum):\n",
    "    IS_RETWEET = \"is_retweet\"\n",
    "    TIMESTAMP = \"tweetcreatedts\"\n",
    "    ID = \"tweetid\"\n",
    "    TEXT = \"text\"\n",
    "\n",
    "    IS_DUPL = \"is_duplicate\"\n",
    "\n",
    "\n",
    "class HashedDataSchema(StrEnum):\n",
    "    HASH = \"hash\"\n",
    "    ID = \"tweetid\"\n",
    "    TIMESTAMP = \"tweetcreatedts\"\n",
    "    DATE = \"date\"\n",
    "\n",
    "def iterate_dataframes(path: str) -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Iterates over all .csv files in path as pd.DataFrame\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "        csvs = [path + x for x in os.listdir(path) if \"csv\" in x]\n",
    "    \n",
    "        for csv in tqdm(csvs):\n",
    "            yield pd.read_csv(csv,  lineterminator='\\n')\n",
    "\n",
    "def aggregate_dataframe(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aggregates dataframe to dict.\n",
    "    \"\"\"\n",
    "    df_la = df.groupby(\"language\").count().reset_index()\n",
    "    languages = df_la['language'].tolist()\n",
    "    lang_counts = df_la['username'].tolist()\n",
    "    df['dupl'] = df.duplicated(subset=\"text\")\n",
    "    languages_dupl = [la + \"_dupl\" for la in languages]\n",
    "    lang_dupl_counts = df.groupby(\"language\")['dupl'].sum().tolist()\n",
    "    unique_user_count = df['userid'].unique().shape[0]\n",
    "    row_count = df.shape[0]\n",
    "    duplicated_count = df.duplicated(subset=\"text\").sum()\n",
    "    date = df.iloc[0]['tweetcreatedts'][:10]\n",
    "    aggregation = {'unique_users': unique_user_count,\n",
    "                  'row_count': row_count,\n",
    "                  'text_duplicated_count': duplicated_count,\n",
    "                  'date':date}\n",
    "    aggregation = {**dict(zip(languages, lang_counts)), **aggregation, **dict(zip(languages_dupl, lang_dupl_counts))}\n",
    "    return aggregation\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def get_all_aggregated_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads or creates all aggregated data.\n",
    "    \"\"\"\n",
    "    path_2023 = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/2023_agg.csv\"\n",
    "    path_2022 = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/2022_agg.csv\"\n",
    "\n",
    "    df_agg_2023 = pd.read_csv(path_2023)\n",
    "    df_agg_2022 = pd.read_csv(path_2022)\n",
    "\n",
    "    df_agg = df_agg = pd.concat([df_agg_2022,df_agg_2023]).fillna(0)\n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def create_hashes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates dataframes containing only hashes and tweetids from not duplicates.\n",
    "    \"\"\"\n",
    "    if OrigDataSchema.IS_RETWEET in df.columns:\n",
    "        df = df[~df.duplicated(subset=OrigDataSchema.TEXT) & ~df[OrigDataSchema.IS_RETWEET]]\n",
    "    else:\n",
    "        df =df[~df.duplicated(subset=OrigDataSchema.TEXT)]\n",
    "        \n",
    "    df_hashed = df[[OrigDataSchema.TEXT, OrigDataSchema.ID,OrigDataSchema.TIMESTAMP]].copy()\n",
    "    \n",
    "    df_hashed[HashedDataSchema.HASH] = df_hashed[OrigDataSchema.TEXT].apply(hash)\n",
    "    \n",
    "    \n",
    "    df_hashed[HashedDataSchema.DATE] = df_hashed[OrigDataSchema.TIMESTAMP].apply(lambda ts: pd.to_datetime(ts[:10]))\n",
    "\n",
    "    df_hashed = df_hashed.drop(OrigDataSchema.TEXT, axis=1).set_index(HashedDataSchema.ID).drop(OrigDataSchema.TIMESTAMP, axis=1)\n",
    "    return df_hashed\n",
    "\n",
    "\n",
    "def aggregate_hash_data(dir_path: str, target_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates aggregated data frame and saves it as csv.\n",
    "    \"\"\"\n",
    "\n",
    "    hashed_dfs = []\n",
    "    for df in iterate_dataframes(dir_path):\n",
    "        hashed = create_hashes(df)\n",
    "        hashed_dfs.append(hashed)\n",
    "\n",
    "    hashed_df = pd.concat(hashed_dfs)\n",
    "    hashed_df.to_csv(target_path)\n",
    "\n",
    "def get_all_hashed_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads all hashed dataframes.\n",
    "    \"\"\"\n",
    "    target_2022 = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/AggregatedData/2022_hashed.csv\"\n",
    "    target_2023 = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/AggregatedData/2023_hashed.csv\"\n",
    "\n",
    "    df_hash_2023 = pd.read_csv(target_2022)\n",
    "    df_hash_2022 = pd.read_csv(target_2023)\n",
    "\n",
    "    df_hashed  = pd.concat([df_hash_2022,df_hash_2023]).sort_values(\"date\")\n",
    "    return df_hashed\n",
    "\n",
    "def iterate_dataframes_path(path: str) -> tuple[Iterator[pd.DataFrame],str]:\n",
    "    \"\"\"\n",
    "    Iterates over all .csv files in path as pd.DataFrame\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "        csvs = [path + x for x in os.listdir(path) if \"csv\" in x]\n",
    "    \n",
    "        for csv in tqdm(csvs):\n",
    "            yield pd.read_csv(csv,  lineterminator='\\n'), csv\n",
    "\n",
    "\n",
    "def add_is_dupl_to_data() -> None:\n",
    "    path_2022_data = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/2022/\"\n",
    "    path_2023_data = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/2023/\"\n",
    "\n",
    "    df_hashed = get_all_hashed_data()\n",
    "    df_hashed = df_hashed[~df_hashed.duplicated('hash')]\n",
    "    \n",
    "    for df, path in iterate_dataframes_path(path_2022_data):\n",
    "        df[OrigDataSchema.IS_DUPL] = df[OrigDataSchema.ID].isin(df_hashed[HashedDataSchema.ID])\n",
    "        df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d4d3c8-dccf-4131-a98a-343496f236af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30110382, 3)\n",
      "(28498970, 3)\n"
     ]
    }
   ],
   "source": [
    "path_2022_data = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/2022/\"\n",
    "target_2022 = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/AggregatedData/2022_hashed.csv\"\n",
    "path_2022_data = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/2022/\"okodil\n",
    "target_2023 = \"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/AggregatedData/2023_hashed.csv\"\n",
    "#aggregate_hash_data(path_2022_data, target_2022)\n",
    "#aggregate_hash_data(path_2023_data, target_2023)\n",
    "df = get_all_hashed_data()\n",
    "print(df.shape)\n",
    "df = df[~df.duplicated('hash')]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b193a8a-c60c-42eb-b360-0de24584a449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c266a7-1acc-422c-ab5b-211d3863bea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ae8e842-91ee-4470-b396-35e59cdcd540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/311 [00:03<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "df_t = next(iterate_dataframes(path_2022_data))\n",
    "\n",
    "df_t[OrigDataSchema.IS_DUPL] = df_t[OrigDataSchema.ID].isin(df[HashedDataSchema.ID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fde8259f-8552-4f1e-b017-d7d3af9c548b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3751567150682496"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t[OrigDataSchema.IS_DUPL].sum()/df_t.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66c6f8d8-e1e0-44e2-93ac-941ece7e7fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122387"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t['is_duplicate'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97dfcd57-8742-4c4e-97ca-76f719bcd10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326229, 30)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23835eef-6971-4687-adc5-9837b55314e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'userid', 'username', 'acctdesc', 'location', 'following',\n",
       "       'followers', 'totaltweets', 'usercreatedts', 'tweetid',\n",
       "       'tweetcreatedts', 'retweetcount', 'text', 'hashtags', 'language',\n",
       "       'coordinates', 'favorite_count', 'is_retweet', 'original_tweet_id',\n",
       "       'original_tweet_userid', 'original_tweet_username',\n",
       "       'in_reply_to_status_id', 'in_reply_to_user_id',\n",
       "       'in_reply_to_screen_name', 'is_quote_status', 'quoted_status_id',\n",
       "       'quoted_status_userid', 'quoted_status_username', 'extractedts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3702a-7be8-41cc-bbf6-1aeab4514776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e75836-f9b4-4d2e-b158-de8a8600458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30110382 entries, 1525264628711936000 to 1631081855142318083\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   tweetcreatedts  object\n",
      " 1   hash            int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 689.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "05b2563a-b9d2-4702-8242-e36967354790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweetid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1525264628711936000</th>\n",
       "      <td>7512023704103795790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525264628715880449</th>\n",
       "      <td>-1632070416298900772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525264628925624325</th>\n",
       "      <td>8837051791152875786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525264628934234120</th>\n",
       "      <td>-8947177930304202717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525264629626191880</th>\n",
       "      <td>-1186198326629031726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631081836112642048</th>\n",
       "      <td>7417735237431925445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631081844127965185</th>\n",
       "      <td>4297768191860994750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631081845906448387</th>\n",
       "      <td>3482933603353950004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631081850507608066</th>\n",
       "      <td>8544894820816301394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631081855142318083</th>\n",
       "      <td>5460335653500132268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30110382 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    hash\n",
       "tweetid                                 \n",
       "1525264628711936000  7512023704103795790\n",
       "1525264628715880449 -1632070416298900772\n",
       "1525264628925624325  8837051791152875786\n",
       "1525264628934234120 -8947177930304202717\n",
       "1525264629626191880 -1186198326629031726\n",
       "...                                  ...\n",
       "1631081836112642048  7417735237431925445\n",
       "1631081844127965185  4297768191860994750\n",
       "1631081845906448387  3482933603353950004\n",
       "1631081850507608066  8544894820816301394\n",
       "1631081855142318083  5460335653500132268\n",
       "\n",
       "[30110382 rows x 1 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
