{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            sanctions\n",
       "1      unjustified_war\n",
       "2                  NaN\n",
       "3        people_killed\n",
       "4            sanctions\n",
       "            ...       \n",
       "101      people_killed\n",
       "102    unjustified_war\n",
       "103    unjustified_war\n",
       "104    unjustified_war\n",
       "105      arms_delivery\n",
       "Name: topics, Length: 106, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel(\"samples_cleaned.xlsx\")[\"topics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 21)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['text', 'translated', 'cleaned_text_translated', 'cleaned_text',\n",
       "       'lemmas', 'adjs_verbs', 'nouns', 'stop_word_removed_lemmas',\n",
       "       'translated_lemmas', 'translated_adjs_verbs', 'translated_nouns',\n",
       "       'translated_stop_word_removed_lemmas', 'topics',\n",
       "       'listed_stop_word_removed_lemmas',\n",
       "       'listed_translated_stop_word_removed_lemmas', 'listed_lemmas',\n",
       "       'listed_adjs_verbs', 'listed_nouns', 'listed_translated_lemmas',\n",
       "       'listed_translated_adjs_verbs', 'listed_translated_nouns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utility import save_csv_with_embeddings, load_samples_with_numpy\n",
    "import pandas as pd\n",
    "from src.explorativ_analysis_05 import split_strings_to_list\n",
    "from src.stop_words import stop_words\n",
    "\n",
    "df = pd.read_excel(\"samples_cleaned.xlsx\")\n",
    "\n",
    "df = df[~df[\"topics\"].isna()]\n",
    "df = df[\n",
    "    df[\"topics\"].isin(\n",
    "        [\"sanctions\", \"unjustified_war\", \"arms_delivery\", \"people_killed\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "to_transform = [\n",
    "    \"lemmas\",\n",
    "    \"adjs_verbs\",\n",
    "    \"nouns\",\n",
    "    \"translated_lemmas\",\n",
    "    \"translated_adjs_verbs\",\n",
    "    \"translated_nouns\",\n",
    "]\n",
    "\n",
    "for col in to_transform:\n",
    "    df[\"listed_\" + col] = df[col].apply(split_strings_to_list)\n",
    "\n",
    "df[\"listed_stop_word_removed_lemmas\"] = df.apply(\n",
    "    lambda x: [\n",
    "        word for word in x[\"listed_lemmas\"] if not word in stop_words[x[\"lang\"]]\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df[\"listed_translated_stop_word_removed_lemmas\"] = df.apply(\n",
    "    lambda x: [\n",
    "        word for word in x[\"listed_translated_lemmas\"] if not word in stop_words[\"en\"]\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df[\"stop_word_removed_lemmas\"] = df[\"listed_stop_word_removed_lemmas\"].str.join(\" \")\n",
    "df[\"translated_stop_word_removed_lemmas\"] = df[\n",
    "    \"listed_translated_stop_word_removed_lemmas\"\n",
    "].str.join(\" \")\n",
    "\n",
    "for col in to_transform:\n",
    "    df[col] = df[col].apply(split_strings_to_list).str.join(\" \")\n",
    "\n",
    "df = df[\n",
    "    [\n",
    "        \"text\",\n",
    "        \"translated\",\n",
    "        \"cleaned_text_translated\",\n",
    "        \"cleaned_text\",\n",
    "        \"lemmas\",\n",
    "        \"adjs_verbs\",\n",
    "        \"nouns\",\n",
    "        \"stop_word_removed_lemmas\",\n",
    "        \"translated_lemmas\",\n",
    "        \"translated_adjs_verbs\",\n",
    "        \"translated_nouns\",\n",
    "        \"translated_stop_word_removed_lemmas\",\n",
    "        \"topics\",\n",
    "        \"listed_stop_word_removed_lemmas\",\n",
    "        \"listed_translated_stop_word_removed_lemmas\",\n",
    "        \"listed_lemmas\",\n",
    "        \"listed_adjs_verbs\",\n",
    "        \"listed_nouns\",\n",
    "        \"listed_translated_lemmas\",\n",
    "        \"listed_translated_adjs_verbs\",\n",
    "        \"listed_translated_nouns\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "top_to_numbers = {\n",
    "    \"sanctions\": 0,\n",
    "    \"unjustified_war\": 1,\n",
    "    \"people_killed\": 2,\n",
    "    \"arms_delivery\": 3,\n",
    "}\n",
    "df[\"topics\"] = df[\"topics\"].apply(lambda x: top_to_numbers[x])\n",
    "print(df.shape)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate ALL the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "models = {}\n",
    "\n",
    "models[\"model_labse\"] = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "models[\"model_parahprase_min\"] = SentenceTransformer(\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "models[\"model_parahprase_max\"] = SentenceTransformer(\n",
    "    \"paraphrase-multilingual-mpnet-base-v2\"\n",
    ")\n",
    "models[\"eng_model\"] = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_spaces = [\n",
    "#     \"lemmas\",\n",
    "#     \"adjs_verbs\",\n",
    "#     \"nouns\",\n",
    "#     \"translated_lemmas\",\n",
    "#     \"translated_adjs_verbs\",\n",
    "#     \"translated_nouns\",\n",
    "# ]\n",
    "\n",
    "lda_spaces = [\n",
    "    \"translated_stop_word_removed_lemmas\",\n",
    "    \"stop_word_removed_lemmas\",\n",
    "]\n",
    "\n",
    "lda_spaces = [\"listed_\" + x for x in lda_spaces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_steps = [\n",
    "#     \"text\",\n",
    "#     \"translated\",\n",
    "#     \"cleaned_text_translated\",\n",
    "#     \"cleaned_text\",\n",
    "#     \"lemmas\",\n",
    "#     \"adjs_verbs\",\n",
    "#     \"nouns\",\n",
    "#     \"translated_lemmas\",\n",
    "#     \"translated_adjs_verbs\",\n",
    "#     \"translated_nouns\",\n",
    "# ]\n",
    "\n",
    "preprocessing_steps = [\n",
    "    \"translated_stop_word_removed_lemmas\",\n",
    "    \"stop_word_removed_lemmas\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['translated_stop_word_removed_lemmas', 'stop_word_removed_lemmas']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:28<00:00, 14.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for preprocessing in tqdm(preprocessing_steps):\n",
    "    for model_name, model in models.items():\n",
    "        df[f\"{model_name}_{preprocessing}_embeddings\"] = df[preprocessing].apply(\n",
    "            model.encode\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'translated', 'cleaned_text_translated', 'cleaned_text',\n",
       "       'lemmas', 'adjs_verbs', 'nouns', 'stop_word_removed_lemmas',\n",
       "       'translated_lemmas', 'translated_adjs_verbs', 'translated_nouns',\n",
       "       'translated_stop_word_removed_lemmas', 'topics', 'listed_lemmas',\n",
       "       'listed_adjs_verbs', 'listed_nouns', 'listed_translated_lemmas',\n",
       "       'listed_translated_adjs_verbs', 'listed_translated_nouns',\n",
       "       'model_labse_translated_stop_word_removed_lemmas_embeddings',\n",
       "       'model_parahprase_min_translated_stop_word_removed_lemmas_embeddings',\n",
       "       'model_parahprase_max_translated_stop_word_removed_lemmas_embeddings',\n",
       "       'eng_model_translated_stop_word_removed_lemmas_embeddings',\n",
       "       'model_labse_stop_word_removed_lemmas_embeddings',\n",
       "       'model_parahprase_min_stop_word_removed_lemmas_embeddings',\n",
       "       'model_parahprase_max_stop_word_removed_lemmas_embeddings',\n",
       "       'eng_model_stop_word_removed_lemmas_embeddings'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_columns = [\n",
    "    \"model_labse_text_embeddings\",\n",
    "    \"model_parahprase_min_text_embeddings\",\n",
    "    \"model_parahprase_max_text_embeddings\",\n",
    "    \"eng_model_text_embeddings\",\n",
    "    \"model_labse_translated_embeddings\",\n",
    "    \"model_parahprase_min_translated_embeddings\",\n",
    "    \"model_parahprase_max_translated_embeddings\",\n",
    "    \"eng_model_translated_embeddings\",\n",
    "    \"model_labse_cleaned_text_translated_embeddings\",\n",
    "    \"model_parahprase_min_cleaned_text_translated_embeddings\",\n",
    "    \"model_parahprase_max_cleaned_text_translated_embeddings\",\n",
    "    \"eng_model_cleaned_text_translated_embeddings\",\n",
    "    \"model_labse_cleaned_text_embeddings\",\n",
    "    \"model_parahprase_min_cleaned_text_embeddings\",\n",
    "    \"model_parahprase_max_cleaned_text_embeddings\",\n",
    "    \"eng_model_cleaned_text_embeddings\",\n",
    "    \"model_labse_lemmas_embeddings\",\n",
    "    \"model_parahprase_min_lemmas_embeddings\",\n",
    "    \"model_parahprase_max_lemmas_embeddings\",\n",
    "    \"eng_model_lemmas_embeddings\",\n",
    "    \"model_labse_adjs_verbs_embeddings\",\n",
    "    \"model_parahprase_min_adjs_verbs_embeddings\",\n",
    "    \"model_parahprase_max_adjs_verbs_embeddings\",\n",
    "    \"eng_model_adjs_verbs_embeddings\",\n",
    "    \"model_labse_nouns_embeddings\",\n",
    "    \"model_parahprase_min_nouns_embeddings\",\n",
    "    \"model_parahprase_max_nouns_embeddings\",\n",
    "    \"eng_model_nouns_embeddings\",\n",
    "    \"model_labse_translated_lemmas_embeddings\",\n",
    "    \"model_parahprase_min_translated_lemmas_embeddings\",\n",
    "    \"model_parahprase_max_translated_lemmas_embeddings\",\n",
    "    \"eng_model_translated_lemmas_embeddings\",\n",
    "    \"model_labse_translated_adjs_verbs_embeddings\",\n",
    "    \"model_parahprase_min_translated_adjs_verbs_embeddings\",\n",
    "    \"model_parahprase_max_translated_adjs_verbs_embeddings\",\n",
    "    \"eng_model_translated_adjs_verbs_embeddings\",\n",
    "    \"model_labse_translated_nouns_embeddings\",\n",
    "    \"model_parahprase_min_translated_nouns_embeddings\",\n",
    "    \"model_parahprase_max_translated_nouns_embeddings\",\n",
    "    \"eng_model_translated_nouns_embeddings\",\n",
    "]\n",
    "\n",
    "len(embeddings_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_columns = [\n",
    "    \"model_labse_translated_stop_word_removed_lemmas_embeddings\",\n",
    "    \"model_parahprase_min_translated_stop_word_removed_lemmas_embeddings\",\n",
    "    \"model_parahprase_max_translated_stop_word_removed_lemmas_embeddings\",\n",
    "    \"eng_model_translated_stop_word_removed_lemmas_embeddings\",\n",
    "    \"model_labse_stop_word_removed_lemmas_embeddings\",\n",
    "    \"model_parahprase_min_stop_word_removed_lemmas_embeddings\",\n",
    "    \"model_parahprase_max_stop_word_removed_lemmas_embeddings\",\n",
    "    \"eng_model_stop_word_removed_lemmas_embeddings\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def map_to_numbers(topic_a: list[int], topic_b: list[int]):\n",
    "    permutations = [\n",
    "        {-1: -1, **{key: val for key, val in zip((0, 1, 2, 3), perm)}}\n",
    "        for perm in list(itertools.permutations([0, 1, 2, 3]))\n",
    "    ]\n",
    "\n",
    "    dist = [\n",
    "        (np.array(topic_a) == np.array([perm[x] for x in topic_b])).sum()\n",
    "        for perm in permutations\n",
    "    ]\n",
    "    permutation = permutations[dist.index(max(dist))]\n",
    "\n",
    "    return (\n",
    "        np.array(topic_a) == np.array([permutation[x] for x in topic_b])\n",
    "    ).sum(), permutation\n",
    "\n",
    "\n",
    "def get_best_topics(topics: list[list[int]], topic: list[int]) -> tuple[list[int], int]:\n",
    "\n",
    "    distances = [map_to_numbers(top, topic)[0] for top in topics]\n",
    "    best_topic = topics[distances.index(max(distances))]\n",
    "\n",
    "    dist, permutation = map_to_numbers(topic, best_topic)\n",
    "\n",
    "    return [permutation[x] for x in best_topic], dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = df[[\"topics\", \"translated\"]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listed_translated_stop_word_removed_lemmas score: 48\n",
      "listed_stop_word_removed_lemmas score: 47\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import stop_words\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from tqdm import tqdm\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    lowercase=False,\n",
    "    stop_words=list(stop_words.STOP_WORDS),\n",
    ")\n",
    "\n",
    "best_lda_topics = []\n",
    "\n",
    "for lda_space in lda_spaces:\n",
    "    X_tf = count_vect.fit_transform(df[lda_space].str.join(\" \"))\n",
    "\n",
    "    num_topics = 4\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=10)\n",
    "\n",
    "    lda_topic_list = []\n",
    "    for _ in range(1000):\n",
    "        lda.fit(X_tf)\n",
    "\n",
    "        lda_topics = [lda.transform(x).argmax() for x in X_tf]\n",
    "        lda_topic_list.append(lda_topics)\n",
    "    best_topic, dist = get_best_topics(lda_topic_list, df[\"topics\"].to_list())\n",
    "\n",
    "    print(f\"{lda_space} score: {dist}\")\n",
    "\n",
    "    best_lda_topics.append({lda_space: best_topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listed_lemmas score: 43\n",
      "listed_adjs_verbs score: 43\n",
      "listed_nouns score: 43\n",
      "listed_translated_lemmas score: 45\n",
      "listed_translated_adjs_verbs score: 46\n",
      "listed_translated_nouns score: 46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.lang.en import stop_words\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from tqdm import tqdm\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    ngram_range=(1, 1),\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    lowercase=False,\n",
    "    stop_words=list(stop_words.STOP_WORDS),\n",
    ")\n",
    "\n",
    "best_lda_topics = []\n",
    "\n",
    "for lda_space in lda_spaces:\n",
    "    X_tf = count_vect.fit_transform(df[lda_space].str.join(\" \"))\n",
    "\n",
    "    num_topics = 4\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, max_iter=10)\n",
    "\n",
    "    lda_topic_list = []\n",
    "    for _ in range(100):\n",
    "        lda.fit(X_tf)\n",
    "\n",
    "        lda_topics = [lda.transform(x).argmax() for x in X_tf]\n",
    "        lda_topic_list.append(lda_topics)\n",
    "    best_topic, dist = get_best_topics(lda_topic_list, df[\"topics\"].to_list())\n",
    "\n",
    "    print(f\"{lda_space} score: {dist}\")\n",
    "\n",
    "    best_lda_topics.append({lda_space: best_topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, topics in {\n",
    "    name: topics for x in best_lda_topics for name, topics in x.items()\n",
    "}.items():\n",
    "    df_topics[name + \"_lda\"] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "random_topics = [\n",
    "    np.random.choice([0, 1, 2, 3], size=(100), replace=True, p=None).tolist()\n",
    "    for _ in range(1000)\n",
    "]\n",
    "best_topic, dist = get_best_topics(random_topics, df[\"topics\"].to_list())\n",
    "df_topics[\"random_topics_best_of_1000\"] = best_topic\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "random_topics = [\n",
    "    np.random.choice([0, 1, 2, 3], size=(100), replace=True, p=None).tolist()\n",
    "    for _ in range(100)\n",
    "]\n",
    "best_topic, dist = get_best_topics(random_topics, df[\"topics\"].to_list())\n",
    "df_topics[\"random_topics_best_of_100\"] = best_topic\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_labse_translated_stop_word_removed_lemmas_embeddings score: 81\n",
      "model_parahprase_min_translated_stop_word_removed_lemmas_embeddings score: 83\n",
      "model_parahprase_max_translated_stop_word_removed_lemmas_embeddings score: 86\n",
      "eng_model_translated_stop_word_removed_lemmas_embeddings score: 83\n",
      "model_labse_stop_word_removed_lemmas_embeddings score: 79\n",
      "model_parahprase_min_stop_word_removed_lemmas_embeddings score: 79\n",
      "model_parahprase_max_stop_word_removed_lemmas_embeddings score: 83\n",
      "eng_model_stop_word_removed_lemmas_embeddings score: 40\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "best_kmeans_topics = {}\n",
    "cluster_model = KMeans(n_clusters=4)\n",
    "topic_model = BERTopic(hdbscan_model=cluster_model)\n",
    "\n",
    "\n",
    "for embedding in embeddings_columns:\n",
    "\n",
    "    kmeans_topics_list = []\n",
    "\n",
    "    for _ in range(100):\n",
    "        topics, probs = topic_model.fit_transform(\n",
    "            df[\"text\"].to_list(),\n",
    "            embeddings=np.stack(df[embedding].to_list(), axis=0),\n",
    "        )\n",
    "        kmeans_topics_list.append(topics)\n",
    "\n",
    "    best_topic, dist = get_best_topics(kmeans_topics_list, df[\"topics\"].to_list())\n",
    "\n",
    "    print(f\"{embedding} score: {dist}\")\n",
    "\n",
    "    best_kmeans_topics[embedding] = best_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/698597202.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"kmeans_\" + name] = topics\n"
     ]
    }
   ],
   "source": [
    "for name, topics in best_kmeans_topics.items():\n",
    "    df_topics[\"kmeans_\" + name] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_labse_translated_stop_word_removed_lemmas_embeddings score: 76\n",
      "model_parahprase_min_translated_stop_word_removed_lemmas_embeddings score: 78\n",
      "model_parahprase_max_translated_stop_word_removed_lemmas_embeddings score: 78\n",
      "eng_model_translated_stop_word_removed_lemmas_embeddings score: 84\n",
      "model_labse_stop_word_removed_lemmas_embeddings score: 72\n",
      "model_parahprase_min_stop_word_removed_lemmas_embeddings score: 74\n",
      "model_parahprase_max_stop_word_removed_lemmas_embeddings score: 80\n",
      "eng_model_stop_word_removed_lemmas_embeddings score: 39\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "\n",
    "best_bert_topics = {}\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=10,\n",
    "    max_cluster_size=40,\n",
    "    min_samples=3,\n",
    ")\n",
    "topic_model = BERTopic(hdbscan_model=hdbscan_model)\n",
    "for embedding in embeddings_columns:\n",
    "\n",
    "    bert_topics_list = []\n",
    "\n",
    "    while len(bert_topics_list) < 100:\n",
    "        topics, probs = topic_model.fit_transform(\n",
    "            df[\"text\"].to_list(),\n",
    "            embeddings=np.stack(df[embedding].to_list(), axis=0),\n",
    "        )\n",
    "        if len(set(topics)) == 5 and (np.array(topics) == -1).sum() < 15:\n",
    "            bert_topics_list.append(topics)\n",
    "\n",
    "        if len(set(topics)) == 4:\n",
    "            topics = [x + 1 for x in topics]\n",
    "            bert_topics_list.append(topics)\n",
    "\n",
    "    best_topic, dist = get_best_topics(bert_topics_list, df[\"topics\"].to_list())\n",
    "\n",
    "    print(f\"{embedding} score: {dist}\")\n",
    "\n",
    "    best_bert_topics[embedding] = best_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/2032561638.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"hdbscan_\" + name] = topics\n",
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/2032561638.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"hdbscan_\" + name] = topics\n",
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/2032561638.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"hdbscan_\" + name] = topics\n",
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/2032561638.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"hdbscan_\" + name] = topics\n",
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/2032561638.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"hdbscan_\" + name] = topics\n",
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/2032561638.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"hdbscan_\" + name] = topics\n",
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/2032561638.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"hdbscan_\" + name] = topics\n",
      "/var/folders/5b/fxkcp_lx10z530m0yx_l70n00000gn/T/ipykernel_29965/2032561638.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_topics[\"hdbscan_\" + name] = topics\n"
     ]
    }
   ],
   "source": [
    "for name, topics in best_bert_topics.items():\n",
    "    df_topics[\"hdbscan_\" + name] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 109)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics.to_csv(\n",
    "    \"/Users/robinfeldmann/TopicAnalysisRUWTweets/DataGitHub/CaseStudyEval/topics_2.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>translated</th>\n",
       "      <th>listed_lemmas_lda</th>\n",
       "      <th>listed_adjs_verbs_lda</th>\n",
       "      <th>listed_nouns_lda</th>\n",
       "      <th>listed_translated_lemmas_lda</th>\n",
       "      <th>listed_translated_adjs_verbs_lda</th>\n",
       "      <th>listed_translated_nouns_lda</th>\n",
       "      <th>random_topics</th>\n",
       "      <th>random_topics_best_of_1000</th>\n",
       "      <th>...</th>\n",
       "      <th>hdbscan_model_parahprase_max_translated_lemmas_embeddings</th>\n",
       "      <th>hdbscan_eng_model_translated_lemmas_embeddings</th>\n",
       "      <th>hdbscan_model_labse_translated_adjs_verbs_embeddings</th>\n",
       "      <th>hdbscan_model_parahprase_min_translated_adjs_verbs_embeddings</th>\n",
       "      <th>hdbscan_model_parahprase_max_translated_adjs_verbs_embeddings</th>\n",
       "      <th>hdbscan_eng_model_translated_adjs_verbs_embeddings</th>\n",
       "      <th>hdbscan_model_labse_translated_nouns_embeddings</th>\n",
       "      <th>hdbscan_model_parahprase_min_translated_nouns_embeddings</th>\n",
       "      <th>hdbscan_model_parahprase_max_translated_nouns_embeddings</th>\n",
       "      <th>hdbscan_eng_model_translated_nouns_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>#UkraineWar: The #EU #sanctions against #Russi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Vogt-Wuchter: \"We are shocked by the war in Uk...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>The Interior Ministry of #Ukraine reported on ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Federal Chancellor STOP imports from Russia i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>#Ukraine\\nEveryone complains about high gas pr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2</td>\n",
       "      <td>Ukraine: at least two civilians killed and 10 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>ALERT - The International Criminal Court issue...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1</td>\n",
       "      <td>🔴 #UkraineRussianWar\\nURGENT‼️\\nThe Internatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>War in #Ukraine: #Russia committed wide range ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3</td>\n",
       "      <td>#Ukraine Ukraine will not receive F-16 even af...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     topics                                         translated  \\\n",
       "0         0  #UkraineWar: The #EU #sanctions against #Russi...   \n",
       "1         1  Vogt-Wuchter: \"We are shocked by the war in Uk...   \n",
       "3         2  The Interior Ministry of #Ukraine reported on ...   \n",
       "4         0  @Federal Chancellor STOP imports from Russia i...   \n",
       "5         0  #Ukraine\\nEveryone complains about high gas pr...   \n",
       "..      ...                                                ...   \n",
       "101       2  Ukraine: at least two civilians killed and 10 ...   \n",
       "102       1  ALERT - The International Criminal Court issue...   \n",
       "103       1  🔴 #UkraineRussianWar\\nURGENT‼️\\nThe Internatio...   \n",
       "104       1  War in #Ukraine: #Russia committed wide range ...   \n",
       "105       3  #Ukraine Ukraine will not receive F-16 even af...   \n",
       "\n",
       "     listed_lemmas_lda  listed_adjs_verbs_lda  listed_nouns_lda  \\\n",
       "0                    0                      0                 2   \n",
       "1                    1                      0                 3   \n",
       "3                    2                      0                 0   \n",
       "4                    1                      1                 0   \n",
       "5                    1                      1                 0   \n",
       "..                 ...                    ...               ...   \n",
       "101                  1                      1                 2   \n",
       "102                  1                      1                 1   \n",
       "103                  1                      1                 1   \n",
       "104                  1                      1                 1   \n",
       "105                  1                      3                 3   \n",
       "\n",
       "     listed_translated_lemmas_lda  listed_translated_adjs_verbs_lda  \\\n",
       "0                               3                                 0   \n",
       "1                               2                                 1   \n",
       "3                               2                                 0   \n",
       "4                               0                                 2   \n",
       "5                               0                                 2   \n",
       "..                            ...                               ...   \n",
       "101                             2                                 2   \n",
       "102                             2                                 1   \n",
       "103                             3                                 1   \n",
       "104                             0                                 1   \n",
       "105                             2                                 3   \n",
       "\n",
       "     listed_translated_nouns_lda  random_topics  random_topics_best_of_1000  \\\n",
       "0                              0              1                           0   \n",
       "1                              1              1                           1   \n",
       "3                              2              2                           2   \n",
       "4                              0              0                           0   \n",
       "5                              2              3                           0   \n",
       "..                           ...            ...                         ...   \n",
       "101                            2              2                           2   \n",
       "102                            2              3                           1   \n",
       "103                            2              1                           1   \n",
       "104                            1              1                           1   \n",
       "105                            3              0                           2   \n",
       "\n",
       "     ...  hdbscan_model_parahprase_max_translated_lemmas_embeddings  \\\n",
       "0    ...                                                  0           \n",
       "1    ...                                                 -1           \n",
       "3    ...                                                  2           \n",
       "4    ...                                                  0           \n",
       "5    ...                                                  0           \n",
       "..   ...                                                ...           \n",
       "101  ...                                                  2           \n",
       "102  ...                                                  1           \n",
       "103  ...                                                  1           \n",
       "104  ...                                                  1           \n",
       "105  ...                                                 -1           \n",
       "\n",
       "     hdbscan_eng_model_translated_lemmas_embeddings  \\\n",
       "0                                                 0   \n",
       "1                                                 3   \n",
       "3                                                 2   \n",
       "4                                                 0   \n",
       "5                                                 0   \n",
       "..                                              ...   \n",
       "101                                               2   \n",
       "102                                               1   \n",
       "103                                               1   \n",
       "104                                               1   \n",
       "105                                              -1   \n",
       "\n",
       "     hdbscan_model_labse_translated_adjs_verbs_embeddings  \\\n",
       "0                                                    3      \n",
       "1                                                    3      \n",
       "3                                                    3      \n",
       "4                                                    2      \n",
       "5                                                    1      \n",
       "..                                                 ...      \n",
       "101                                                  0      \n",
       "102                                                  1      \n",
       "103                                                  1      \n",
       "104                                                  1      \n",
       "105                                                  2      \n",
       "\n",
       "     hdbscan_model_parahprase_min_translated_adjs_verbs_embeddings  \\\n",
       "0                                                    3               \n",
       "1                                                    3               \n",
       "3                                                    0               \n",
       "4                                                    0               \n",
       "5                                                    1               \n",
       "..                                                 ...               \n",
       "101                                                  0               \n",
       "102                                                  0               \n",
       "103                                                  0               \n",
       "104                                                  3               \n",
       "105                                                  1               \n",
       "\n",
       "     hdbscan_model_parahprase_max_translated_adjs_verbs_embeddings  \\\n",
       "0                                                    1               \n",
       "1                                                    1               \n",
       "3                                                    1               \n",
       "4                                                    0               \n",
       "5                                                    0               \n",
       "..                                                 ...               \n",
       "101                                                  1               \n",
       "102                                                  1               \n",
       "103                                                  1               \n",
       "104                                                  3               \n",
       "105                                                  0               \n",
       "\n",
       "     hdbscan_eng_model_translated_adjs_verbs_embeddings  \\\n",
       "0                                                    3    \n",
       "1                                                    3    \n",
       "3                                                    2    \n",
       "4                                                    0    \n",
       "5                                                    0    \n",
       "..                                                 ...    \n",
       "101                                                  2    \n",
       "102                                                  1    \n",
       "103                                                  1    \n",
       "104                                                  3    \n",
       "105                                                  0    \n",
       "\n",
       "     hdbscan_model_labse_translated_nouns_embeddings  \\\n",
       "0                                                  0   \n",
       "1                                                  1   \n",
       "3                                                  3   \n",
       "4                                                  0   \n",
       "5                                                  0   \n",
       "..                                               ...   \n",
       "101                                                2   \n",
       "102                                                1   \n",
       "103                                                1   \n",
       "104                                                0   \n",
       "105                                                3   \n",
       "\n",
       "     hdbscan_model_parahprase_min_translated_nouns_embeddings  \\\n",
       "0                                                    3          \n",
       "1                                                    3          \n",
       "3                                                    2          \n",
       "4                                                    0          \n",
       "5                                                    0          \n",
       "..                                                 ...          \n",
       "101                                                  3          \n",
       "102                                                  1          \n",
       "103                                                  1          \n",
       "104                                                  1          \n",
       "105                                                  3          \n",
       "\n",
       "     hdbscan_model_parahprase_max_translated_nouns_embeddings  \\\n",
       "0                                                    0          \n",
       "1                                                    1          \n",
       "3                                                    2          \n",
       "4                                                    0          \n",
       "5                                                    0          \n",
       "..                                                 ...          \n",
       "101                                                  3          \n",
       "102                                                  0          \n",
       "103                                                  0          \n",
       "104                                                  1          \n",
       "105                                                  1          \n",
       "\n",
       "     hdbscan_eng_model_translated_nouns_embeddings  \n",
       "0                                                0  \n",
       "1                                               -1  \n",
       "3                                                2  \n",
       "4                                                0  \n",
       "5                                                0  \n",
       "..                                             ...  \n",
       "101                                              2  \n",
       "102                                              1  \n",
       "103                                              1  \n",
       "104                                              1  \n",
       "105                                              2  \n",
       "\n",
       "[100 rows x 91 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
