{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Text Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Text Preparation iam sticking to methods that are defined in Prof. Albrechts Book: [Blueprints for Text Analytics Using Python](https://learning.oreilly.com/library/view/blueprints-for-text/9781492074076/ch04.html#idm46749280280440).\n",
    "I will be using some of his python code original or slightly modified from here [Github Repo](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch04/Data_Preparation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of text preparation is cleaning the text. Espescially tweets can be very dirty, containing weird punctuation, symbols smileys and others.\n",
    "The cleaning function from Prof. Albrechts lectures materials that is slightly modified for this context here will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.text_preparation_04 import clean\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Language_min/de.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See an example on how the cleaning modifies the text from the german tweets. As you can see even though the cleaning improves the texts a little bit, the texts are still not very clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolut sehenswerte Einordnung der Hintergründe und aktuellen Ereignisse rund um den #UkraineKrieg. https://t.co/okaiNsguFA\n",
      "\n",
      "absolut sehenswerte einordnung der hintergründe und aktuellen ereignisse rund um den  ukrainekrieg.\n"
     ]
    }
   ],
   "source": [
    "df_sample = df.sample(10)\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(clean)\n",
    "\n",
    "print(df_sample[['text']].iloc[0]['text'])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(df_sample[['cleaned_text']].iloc[0]['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is lemma extraction. Here lemmas, nouns, adjectives, verbs and emojis are extracted and lemmatised. I again use a modified version of Prof. Albrechts function from his Natural Language Processing Lectures. \n",
    "See an example for how the process works on german tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dabei wird indes auch erkennbar, dass \"der westen\" die neuen realitäten nicht erkennen kann oder will.  china ist zur  supermacht geworden und schließt als solche diplomatische abkommen mit seinen nachbarn. egal ob  australien, die  usa oder  eu das wollen oder nicht.\n",
      "\n",
      "lemmas    [dabei, indes, auch, erkennbar, Westen, neu, Realität, erkennen, wollen, China, zu, Supermacht, schließen, als, diplomatisch, Abkommen, mit, Nachbar, egal, Australien, USA, EU, der, wollen]\n",
      "Name: 1083033, dtype: object\n",
      "nouns    [Westen, Realität, China, Supermacht, Abkommen, Nachbar, Australien, USA, EU]\n",
      "Name: 1083033, dtype: object\n",
      "adjs_verbs    [neu, erkennen, wollen, schließen, diplomatisch, wollen]\n",
      "Name: 1083033, dtype: object\n",
      "emojis    []\n",
      "Name: 1083033, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "from src.text_preparation_04 import add_lemmas_to_df\n",
    "df_sample = df.sample(10)\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(clean)\n",
    "\n",
    "\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "df_sample = add_lemmas_to_df(df_sample, nlp)\n",
    "\n",
    "print(df_sample.iloc[0]['cleaned_text'])\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(df_sample.iloc[0][['lemmas']])\n",
    "print(df_sample.iloc[0][['nouns']])\n",
    "print(df_sample.iloc[0][['adjs_verbs']])\n",
    "print(df_sample.iloc[0][['emojis']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma Extraction \n",
    "In the following section for every language the lemmas will be extracted and saved with the original text in /Lemmas . This is a very computional heavy calculation. On the biggest english tweet dataset it took my poor computer more then 24 hours to do this. So don't rerun this cells! Saving dfs hase been commented out to prevent overriding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lemmas for Germany Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4561a9b858f41c2bb05bafe421dc7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1580284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe7a0540472449193c98212c4362e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15803 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "from src.text_preparation_04 import clean, add_lemmas_to_df\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Language_min_dedupl/de.csv\")\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean)\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "df = add_lemmas_to_df(df, nlp)\n",
    "\n",
    "#df[['tweetid','tweetcreatedts','lemmas','adjs_verbs','nouns', 'entities', 'emojis']].to_csv(\"\n",
    "\n",
    "\n",
    "\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lemmas for English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63dc92aea954432a92ae6b862baa3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12116412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db380fa78e31486eac36ad136c818055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "from src.text_preparation_04 import clean, add_lemmas_to_df\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Language_min_dedupl/en.csv\")\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "df = add_lemmas_to_df(df, nlp)\n",
    "\n",
    "#df[['tweetid','tweetcreatedts','lemmas','adjs_verbs','nouns', 'entities', 'emojis']].to_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Lemmas/en.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lemmas for Russian Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "from src.text_preparation_04 import clean, add_lemmas_to_df\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Language_min_dedupl/ru.csv\")\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean)\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "df = add_lemmas_to_df(df, nlp)\n",
    "\n",
    "#df[['tweetid','tweetcreatedts','lemmas','adjs_verbs','nouns', 'entities', 'emojis']].to_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Lemmas/ru.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lemmas for Spanish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1002517/1002517 [01:34<00:00, 10578.34it/s]\n",
      "100%|██████████| 10026/10026 [4:15:56<00:00,  1.53s/it]     \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "from src.text_preparation_04 import clean, add_lemmas_to_df\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Language_min_dedupl/es.csv\")\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean)\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "df = add_lemmas_to_df(df, nlp)\n",
    "\n",
    "#df[['tweetid','tweetcreatedts','lemmas','adjs_verbs','nouns', 'entities', 'emojis']].to_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Lemmas/es.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lemmas for Italian Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1178277/1178277 [01:47<00:00, 10922.41it/s]\n",
      "100%|██████████| 11783/11783 [16:55:06<00:00,  5.17s/it]      \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "from src.text_preparation_04 import clean, add_lemmas_to_df\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Language_min_dedupl/it.csv\")\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean)\n",
    "nlp = spacy.load(\"it_core_news_sm\")\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "df = add_lemmas_to_df(df, nlp)\n",
    "\n",
    "#df[['tweetid','tweetcreatedts','lemmas','adjs_verbs','nouns', 'entities', 'emojis']].to_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Lemmas/it.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lemmas for French Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1049599/1049599 [01:58<00:00, 8849.74it/s]\n",
      "100%|██████████| 10496/10496 [5:19:09<00:00,  1.82s/it]      \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "from src.text_preparation_04 import clean, add_lemmas_to_df\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Language_min_dedupl/fr.csv\")\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean)\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "df = add_lemmas_to_df(df, nlp)\n",
    "\n",
    "df[['tweetid','tweetcreatedts','lemmas','adjs_verbs','nouns', 'entities', 'emojis']].to_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Lemmas/fr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Lemmas for Ukrainian Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 899934/899934 [01:01<00:00, 14590.00it/s]\n",
      "100%|██████████| 9000/9000 [7:26:01<00:00,  2.97s/it]       \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "from src.text_preparation_04 import clean, add_lemmas_to_df\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Language_min_dedupl/uk.csv\")\n",
    "df['cleaned_text'] = df['text'].progress_apply(clean)\n",
    "nlp = spacy.load(\"uk_core_news_sm\")\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "df = add_lemmas_to_df(df, nlp)\n",
    "\n",
    "df[['tweetid','tweetcreatedts','lemmas','adjs_verbs','nouns', 'entities', 'emojis']].to_csv(\"/Users/robinfeldmann/TopicAnalysisRUWTweets/Data/Lemmas/uk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Explorativ Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter_nouns = Counter()\n",
    "counter_verbs = Counter()\n",
    "counter_lemmas = Counter()\n",
    "counter_emojis = Counter()\n",
    "for ind, row in df_sample.iterrows():\n",
    "    counter_nouns.update(row['nouns'])\n",
    "    counter_verbs.update(row['adjs_verbs'])\n",
    "    counter_lemmas.update(row['lemmas'])\n",
    "    counter_emojis.update(row['emojis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9331"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(counter_nouns.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('russia', 303),\n",
       " ('ukraine', 298),\n",
       " ('war', 140),\n",
       " ('putin', 130),\n",
       " ('news', 69),\n",
       " ('people', 66),\n",
       " ('usa', 65),\n",
       " ('nato', 60),\n",
       " ('biden', 47),\n",
       " ('country', 44)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter_nouns.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ukraine', 161),\n",
       " ('russian', 139),\n",
       " ('ukrainian', 84),\n",
       " ('have', 80),\n",
       " ('say', 65),\n",
       " ('go', 50),\n",
       " ('more', 44),\n",
       " ('ukrainewar', 44),\n",
       " ('do', 43),\n",
       " ('new', 42)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter_verbs.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
